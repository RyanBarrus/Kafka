package com.kafka;

import java.time.Duration;
import java.util.Arrays;
import java.util.Properties;

import org.apache.kafka.clients.CommonClientConfigs;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.common.config.SaslConfigs;

import org.apache.avro.generic.GenericData.Record;
import io.confluent.kafka.serializers.KafkaAvroDeserializerConfig;

import java.sql.Connection;
import java.sql.DatabaseMetaData;
import java.sql.DriverManager;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;



public class ConfluentAvroConsumer {
	
	private static final String jdbcDriver = "com.microsoft.sqlserver.jdbc.SQLServerDriver";
	private static final String jdbcURL = "jdbc:sqlserver://DWR01;databasename=Work;integratedSecurity=true;";
	
	public static void main(String[] args) {
        Properties props = new Properties();
        props.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, "pkc-41973.westus2.azure.confluent.cloud:9092");
        props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_SSL");
        props.put(SaslConfigs.SASL_MECHANISM, "PLAIN");
        props.put(SaslConfigs.SASL_JAAS_CONFIG, "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"" 
                + GeneralConfig.confluentKey + "\" password=\"" + GeneralConfig.confluentSecret + "\";");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,  "io.confluent.kafka.serializers.KafkaAvroDeserializer");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "consumerGroup1");
        
        props.put(KafkaAvroDeserializerConfig.SCHEMA_REGISTRY_URL_CONFIG, "https://psrc-4r0k9.westus2.azure.confluent.cloud");
        props.put(KafkaAvroDeserializerConfig.BASIC_AUTH_CREDENTIALS_SOURCE,"USER_INFO");
        props.put(KafkaAvroDeserializerConfig.USER_INFO_CONFIG, GeneralConfig.confluentSchemaRegistryKey 
        		+ ":" + GeneralConfig.confluentSchemaRegistrySecret);

        KafkaConsumer<String, Record> consumer = new KafkaConsumer<String, Record>(props);
        
        consumer.subscribe(Arrays.asList("Race"));
        
        
         

        try {
			while (true) {
				ConsumerRecords<String, Record> records = consumer.poll(Duration.ofMillis(1000));
				for (ConsumerRecord<String, Record> record : records) {
					
					Record race = record.value();
					
					
					try {
						Connection conn = DriverManager.getConnection(jdbcURL);
						
						PreparedStatement stmt = conn.prepareStatement("INSERT INTO work.dbo.TempRyanKafka VALUES (?,?,?,?,?,?)");
						
						
						

						stmt.setInt(1,(int) race.get("Place"));
						stmt.setString(2,(String) race.get("Name"));
						stmt.setString(3,(String) race.get("Year"));
						stmt.setString(4,(String) race.get("Team"));
						stmt.setString(5,(String) race.get("AverageTime"));
						stmt.setString(6,(String) race.get("Time"));
						
						System.out.println("about to insert");
					
						stmt.execute();
						
						System.out.println("succesful insert");
						 
					} catch (SQLException e1) {
						e1.printStackTrace();
					}
					
					System.out.println("Inserted into the database: " + race);

				}
				
			}
		} catch (Exception e) {
			System.out.println(("Exception occured while consuming messages" + e));
		}finally {
			consumer.close();
		}
        
    }
	
}
